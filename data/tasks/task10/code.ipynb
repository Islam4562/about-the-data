from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, desc, to_timestamp

# 1. Инициализация Spark-сессии
spark = SparkSession.builder \
    .appName("WebLogsAnalysis") \
    .getOrCreate()

# 2. Загрузка CSV-файла с логами
df = spark.read.csv("web_logs.csv", header=True, inferSchema=True)

# 3. Преобразуем поле timestamp в datetime формат
df = df.withColumn("timestamp", to_timestamp("timestamp"))

# 4. Анализ: количество запросов по каждому IP
df.groupBy("ip") \
    .agg(count("*").alias("total_requests")) \
    .orderBy(desc("total_requests")) \
    .show(10, truncate=False)

# 5. Анализ: самые популярные URL
df.groupBy("url") \
    .agg(count("*").alias("url_hits")) \
    .orderBy(desc("url_hits")) \
    .show(10, truncate=False)

# 6. Анализ: распределение по HTTP-статусам
df.groupBy("status") \
    .agg(count("*").alias("status_count")) \
    .orderBy(desc("status_count")) \
    .show()

# 7. Завершение
spark.stop()
